
# WinterCapstone2017
Ryerson Capstone Project

#Initially it will extract 1000 tweets  by region with a radio of 50 milles
#Etobicoke, Toronto, ON, Canada
#Latitude and longitude coordinates are: 43.620495, -79.513199.
RS_Etobicoke = searchTwitter("Real Estate", geocode="43.620495,-79.513199,50mi",lang = "en" , n=1000)
RS_Etobicoke
head(RS_Etobicoke)
#High Park, Toronto, ON, Canada
#43.645485,-79.464752
RS_HighPark = searchTwitter("Real Estate", geocode="43.645485,-79.464752,50mi", lang = "en" , n=1000)
RS_HighPark
#North York, Toronto, ON, Canada
#43.761539, -79.411079
RS_NorthYork= searchTwitter("Real Estate", geocode="43.761539,-79.411079,50mi", lang = "en" , n=1000)
RS_NorthYork
#Trinity Bellwoods Park, Toronto, ON, Canada
#43.646355,-79.413582
RS_TrinityBellwoods= searchTwitter("Real Estate", geocode="43.646355,-79.413582,50mi", lang = "en" , n=1000)
RS_TrinityBellwoods
#East York, Toronto, ON, Canada
#43.691200,-79.341667
RS_EastYork= searchTwitter("Real Estate", geocode="43.691200,-79.341667,50mi", lang = "en" , n=1000)
RS_EastYork
#Port Lands, Toronto, ON, Canada
#43.649738,-79.339485
RS_PortLands= searchTwitter("Real Estate", geocode="43.649738,-79.339485,50mi", lang = "en" , n=1000)
RS_PortLands
#Toronto City Hall, Toronto, ON, Canada
#43.653908,-79.384293
RS_TorontoCity= searchTwitter("Real Estate", geocode="43.653908,-79.384293,50mi", lang = "en" , n=1000)
RS_TorontoCity

#twListToDF:Â Convert twitter data into a data frame
# convert tweets into a data frame
tdata_ET_df = twListToDF(RS_Etobicoke)
tdata_HP_df = twListToDF(RS_HighPark)
tdata_NY_df = twListToDF(RS_NorthYork)
tdata_TB_df = twListToDF(RS_TrinityBellwoods)
tdata_EY_df = twListToDF(RS_EastYork)
tdata_P_df = twListToDF(RS_PortLands)
tdata_TC_df = twListToDF(RS_TorontoCity)

#Chek the data 
class(tdata_ET_df) 
class(tdata_HP_df)
class(tdata_NY_df)
class(tdata_TB_df)
class(tdata_EY_df)
class(tdata_P_df)
class(tdata_TC_df)

#Summary
summary(tdata_ET_df)
summary(tdata_HP_df)
summary(tdata_NY_df)
summary(tdata_TB_df)
summary(tdata_EY_df)
summary(tdata_P_df)
summary(tdata_TC_df)

#str
str(tdata_ET_df)
str(tdata_HP_df)
str(tdata_NY_df)
str(tdata_TB_df)
str(tdata_EY_df)
str(tdata_P_df)
str(tdata_TC_df)

#head
head(tdata_ET_df)
head(tdata_HP_df)
head(tdata_NY_df)
head(tdata_TB_df)
head(tdata_EY_df)
head(tdata_P_df)
head(tdata_TC_df)

#tail
tail(tdata_ET_df)
tail(tdata_HP_df)
tail(tdata_NY_df)
tail(tdata_TB_df)
tail(tdata_EY_df)
tail(tdata_P_df)
tail(tdata_TC_df)

#length
length(tdata_ET_df)
length(tdata_HP_df)
length(tdata_NY_df)
length(tdata_TB_df)
length(tdata_EY_df)
length(tdata_P_df)
length(tdata_TC_df)

#dim
dim(tdata_ET_df)
dim(tdata_HP_df)
dim(tdata_NY_df)
dim(tdata_TB_df)
dim(tdata_EY_df)
dim(tdata_P_df)
dim(tdata_TC_df)

#nrow
nrow(tdata_ET_df)
nrow(tdata_HP_df)
nrow(tdata_NY_df)
nrow(tdata_TB_df)
nrow(tdata_EY_df)
nrow(tdata_P_df)
nrow(tdata_TC_df)

#ncol
ncol(tdata_ET_df)
ncol(tdata_HP_df)
ncol(tdata_NY_df)
ncol(tdata_TB_df)
ncol(tdata_EY_df)
ncol(tdata_P_df)
ncol(tdata_TC_df)

#data type of each feauture
sapply(tdata_ET_df, class)
sapply(tdata_HP_df, class)
sapply(tdata_NY_df, class)
sapply(tdata_TB_df, class)
sapply(tdata_EY_df, class)
sapply(tdata_P_df, class)
sapply(tdata_TC_df, class)


#MAKe a wordcloud
#Step 1:Load all the required packages

install.packages("twitteR", dependencies=TRUE) 
install.packages("tm", dependencies=TRUE) 
install.packages("wordcloud", dependencies=TRUE) 
install.packages("RColorBrewer", dependencies=TRUE) 


library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)

# data (from twitter, no dataframe)
data_ET<-RS_Etobicoke
data_HP<-RS_HighPark
data_NY<-RS_NorthYork
data_TB<-RS_TrinityBellwoods
data_EY<-RS_EastYork
data_P<-RS_PortLands
data_TC<-RS_TorontoCity

#FOR Etobicoke

#Step 2:Extract the text from the tweets in a vector

text_ET = sapply(data_ET, function(x) x$getText())


#Step 3:Construct the lexical Corpus and the Term Document Matrix
#use the function Corpus to create the corpus, 
#and the function VectorSource to indicate that 
#the text is in the character vector mach_text. 
#In order to create the term-document matrix 
#apply different transformation such as 
#removing numbers, punctuation symbols, lower case, etc.

# create a corpus
corpus_ET = Corpus(VectorSource(text_ET))

# create document term matrix applying some transformations
tdm_ET = TermDocumentMatrix(corpus_ET,
                         control = list(removePunctuation = TRUE,
                                        stopwords = c("Real", "State", stopwords("english")),
                                        removeNumbers = TRUE,tolower = TRUE))


#Step 4: Obtain words and their frequencies

# define tdm_ET as matrix
m_ET = as.matrix(tdm_ET)
# get word counts in decreasing order
word_freqs_ET = sort(rowSums(m_ET), decreasing=TRUE)
# create a data frame with words and their frequencies
dm_ET = data.frame(word=names(word_freqs_ET), freq=word_freqs_ET)


#Step 5:  plot the wordcloud

# plot wordcloud
wordcloud(dm_E$word, dm_E$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))

#Show frequent words
freq.term<-findFreqTerms(dm_E$word, lowfreq=15)

#PLot

install.packages("ggplot2", dependencies=TRUE) 
ggplot(dm_E, ae(x=term, y=freq)) + geom_bar(stat = 'identity")+
xlab("Terms")+ylab("Count") + coord_flip()
















